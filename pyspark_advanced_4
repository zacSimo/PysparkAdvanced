{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:11:34.071838Z","iopub.execute_input":"2024-11-22T11:11:34.072381Z","iopub.status.idle":"2024-11-22T11:11:44.328138Z","shell.execute_reply.started":"2024-11-22T11:11:34.072323Z","shell.execute_reply":"2024-11-22T11:11:44.326998Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.3)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"pyspark_practice_4\").getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:11:44.331261Z","iopub.execute_input":"2024-11-22T11:11:44.331845Z","iopub.status.idle":"2024-11-22T11:11:44.341880Z","shell.execute_reply.started":"2024-11-22T11:11:44.331776Z","shell.execute_reply":"2024-11-22T11:11:44.340789Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"spark.sparkContext.setLogLevel(\"ERROR\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:17:31.117687Z","iopub.execute_input":"2024-11-22T11:17:31.118611Z","iopub.status.idle":"2024-11-22T11:17:31.126376Z","shell.execute_reply.started":"2024-11-22T11:17:31.118568Z","shell.execute_reply":"2024-11-22T11:17:31.125189Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## SHUFFLING - SKEWNESS - data distribution across the partitions","metadata":{}},{"cell_type":"markdown","source":"### **1. Run a schuffle *groupByKey* to see how the skew effect computation resources**","metadata":{}},{"cell_type":"code","source":"data_sample = [(1,4),(2,2),(2,1),(3,5),(2,5),(2,10),(2,7),(3,4),(2,1),(2,4),(4,4)]\nrdd_sample = spark.sparkContext.parallelize(data_sample, 3)\nrdd_sample.glom().collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:11:44.343263Z","iopub.execute_input":"2024-11-22T11:11:44.343765Z","iopub.status.idle":"2024-11-22T11:11:44.666071Z","shell.execute_reply.started":"2024-11-22T11:11:44.343634Z","shell.execute_reply":"2024-11-22T11:11:44.664768Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[[(1, 4), (2, 2), (2, 1)],\n [(3, 5), (2, 5), (2, 10)],\n [(2, 7), (3, 4), (2, 1), (2, 4), (4, 4)]]"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"rdd_sample_grouped = rdd_sample.groupByKey()\n\nfor item in rdd_sample_grouped.collect():\n    print(item[0], [value for value in item[1]])\n\n# Output:\n\n# 3 [5, 4]\n# 1 [4]\n# 4 [4]\n# 2 [2, 1, 5, 10, 7, 1, 4] --> Skewness uneven data distribution\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:11:44.667544Z","iopub.execute_input":"2024-11-22T11:11:44.668088Z","iopub.status.idle":"2024-11-22T11:11:45.187635Z","shell.execute_reply.started":"2024-11-22T11:11:44.668026Z","shell.execute_reply":"2024-11-22T11:11:45.186682Z"}},"outputs":[{"name":"stdout","text":"3 [5, 4]\n1 [4]\n4 [4]\n2 [2, 1, 5, 10, 7, 1, 4]\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# show partitions (result = 4)\nrdd_sample_grouped.glom().collect()\n\n# output:\n# [[(3, <pyspark.resultiterable.ResultIterable at 0x7cb6b85026e0>)],\n#  [(1, <pyspark.resultiterable.ResultIterable at 0x7cb6b8502ec0>),\n#   (4, <pyspark.resultiterable.ResultIterable at 0x7cb6b8503760>)],\n#  [(2, <pyspark.resultiterable.ResultIterable at 0x7cb6b85031f0>)]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:11:45.190851Z","iopub.execute_input":"2024-11-22T11:11:45.191282Z","iopub.status.idle":"2024-11-22T11:11:45.469822Z","shell.execute_reply.started":"2024-11-22T11:11:45.191231Z","shell.execute_reply":"2024-11-22T11:11:45.468806Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"[[(3, <pyspark.resultiterable.ResultIterable at 0x7cb6b910f9d0>)],\n [(1, <pyspark.resultiterable.ResultIterable at 0x7cb66866dc00>),\n  (4, <pyspark.resultiterable.ResultIterable at 0x7cb66866e800>)],\n [(2, <pyspark.resultiterable.ResultIterable at 0x7cb66866fc10>)]]"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"### **Solution use *SALTING***","metadata":{}},{"cell_type":"code","source":"# redistribute data by adding random value\n\nimport numpy as np\nimport random\n\n# generate skew values\nkey_1 = ['a'] * 10\nkey_2 = ['b'] * 6000000\nkey_3 = ['c'] * 800\nkey_4 = ['d'] * 10000\n\nkeys = key_1 + key_2 + key_3 + key_4\n\nrandom.shuffle(keys)\n\nvalue_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\nvalue_2 = list(np.random.randint(low = 1, high = 100, size = len(key_2)))\nvalue_3 = list(np.random.randint(low = 1, high = 100, size = len(key_3)))\nvalue_4 = list(np.random.randint(low = 1, high = 100, size = len(key_4)))\n\nvalues = value_1 + value_2 + value_3 + value_4\n\npair_skew = list(zip(keys, values))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:11:45.471012Z","iopub.execute_input":"2024-11-22T11:11:45.471419Z","iopub.status.idle":"2024-11-22T11:11:51.656312Z","shell.execute_reply.started":"2024-11-22T11:11:45.471371Z","shell.execute_reply":"2024-11-22T11:11:51.655377Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# load skew values into RDD\nrdd = spark.sparkContext.parallelize(pair_skew, 8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:11:51.657576Z","iopub.execute_input":"2024-11-22T11:11:51.657948Z","iopub.status.idle":"2024-11-22T11:12:14.656755Z","shell.execute_reply.started":"2024-11-22T11:11:51.657914Z","shell.execute_reply":"2024-11-22T11:12:14.652744Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"%time\nrdd_group = rdd.groupByKey().cache()\n# run a simple data transformation on skewed data and see the result on cluster DAG\nrdd_group.map(lambda pair: (pair[0], [i for i in pair[1]])).count()\n\n# Output:\n# 4 --> we have 4 partitions with value that represent number of keys in groupByKey","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:12:14.658075Z","iopub.execute_input":"2024-11-22T11:12:14.658499Z","iopub.status.idle":"2024-11-22T11:13:07.330167Z","shell.execute_reply.started":"2024-11-22T11:12:14.658444Z","shell.execute_reply":"2024-11-22T11:13:07.329178Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 6 µs, sys: 1 µs, total: 7 µs\nWall time: 10.7 µs\n","output_type":"stream"},{"name":"stderr","text":"24/11/22 11:12:14 WARN TaskSetManager: Stage 21 contains a task of very large size (16952 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"### *groupByKey* ** Migrate data skewness: *SALTING***","metadata":{}},{"cell_type":"code","source":"#  We define the salting method for data redistribution\ndef salting(val):\n    tmp = val + \"_\" + str(random.randint(0,5)) # increasing the random ('high') we have better results \n    return tmp\n\nsalting(\"10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:13:07.331363Z","iopub.execute_input":"2024-11-22T11:13:07.331964Z","iopub.status.idle":"2024-11-22T11:13:07.341360Z","shell.execute_reply.started":"2024-11-22T11:13:07.331914Z","shell.execute_reply":"2024-11-22T11:13:07.340198Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'10_1'"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# Apply salting method to skewed data for data redistribution\n%time\nrdd_salting = rdd.map(lambda x: (salting(x[0]), x[1]))\n\nrdd_grouped = rdd_salting.groupByKey().cache()\n# run a simple data transformation on skewed data and see the result on cluster (job/DAG/stage)\nrdd_grouped.map(lambda pair: (pair[0], [i for i in pair[1]])).count()\n\n# Output:\n# 23 --> we get 23 partitions after repartitioning with randaom data\n# increasing the random value ('high') in 'salting' method, we can have better results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:13:07.343466Z","iopub.execute_input":"2024-11-22T11:13:07.344403Z","iopub.status.idle":"2024-11-22T11:13:46.415359Z","shell.execute_reply.started":"2024-11-22T11:13:07.344352Z","shell.execute_reply":"2024-11-22T11:13:46.414137Z"}},"outputs":[{"name":"stderr","text":"24/11/22 11:13:07 WARN TaskSetManager: Stage 23 contains a task of very large size (16952 KiB). The maximum recommended task size is 1000 KiB.\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 5 µs, sys: 0 ns, total: 5 µs\nWall time: 10 µs\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"### **2. Run a schuffle *sortByKey* to see how the skew effect computation resources**","metadata":{}},{"cell_type":"code","source":"rdd_sort = rdd.sortByKey(ascending=False, numPartitions=4)\nrdd_sort.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:13:46.417316Z","iopub.execute_input":"2024-11-22T11:13:46.417763Z","iopub.status.idle":"2024-11-22T11:14:38.582289Z","shell.execute_reply.started":"2024-11-22T11:13:46.417709Z","shell.execute_reply":"2024-11-22T11:14:38.581036Z"}},"outputs":[{"name":"stderr","text":"24/11/22 11:13:46 WARN TaskSetManager: Stage 25 contains a task of very large size (16952 KiB). The maximum recommended task size is 1000 KiB.\n24/11/22 11:13:48 WARN TaskSetManager: Stage 26 contains a task of very large size (16952 KiB). The maximum recommended task size is 1000 KiB.\n24/11/22 11:13:50 WARN TaskSetManager: Stage 27 contains a task of very large size (16952 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"6010810"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"### *sortByKey* **Migrate data skewness: *SALTING***","metadata":{}},{"cell_type":"code","source":"rdd_sorted = rdd_salting.sortByKey(ascending=False, numPartitions=4)\nrdd_sorted.count()\n\n# Output:\n\n# We have the same size data in the result\n# The redistribution data in the partitions are better than the previous one.\n# That is confirmed by the time taken by the computation, 2 sec # 18 sec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T11:14:38.584112Z","iopub.execute_input":"2024-11-22T11:14:38.584445Z","iopub.status.idle":"2024-11-22T11:15:18.683758Z","shell.execute_reply.started":"2024-11-22T11:14:38.584411Z","shell.execute_reply":"2024-11-22T11:15:18.682486Z"}},"outputs":[{"name":"stderr","text":"24/11/22 11:14:38 WARN TaskSetManager: Stage 29 contains a task of very large size (16952 KiB). The maximum recommended task size is 1000 KiB.\n24/11/22 11:14:44 WARN TaskSetManager: Stage 30 contains a task of very large size (16952 KiB). The maximum recommended task size is 1000 KiB.\n24/11/22 11:14:51 WARN TaskSetManager: Stage 31 contains a task of very large size (16952 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"6010810"},"metadata":{}}],"execution_count":42}]}