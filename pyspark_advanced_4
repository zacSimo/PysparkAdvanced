{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:16:55.512499Z","iopub.execute_input":"2024-11-22T09:16:55.512894Z","iopub.status.idle":"2024-11-22T09:17:42.550056Z","shell.execute_reply.started":"2024-11-22T09:16:55.512856Z","shell.execute_reply":"2024-11-22T09:17:42.548788Z"}},"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=0b7ca385b1548267263f331c4999261cb62b660ac4b6085b445640c23186e5fe\n  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"pyspark_practice_4\").getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:17:42.552544Z","iopub.execute_input":"2024-11-22T09:17:42.553067Z","iopub.status.idle":"2024-11-22T09:17:48.200568Z","shell.execute_reply.started":"2024-11-22T09:17:42.552992Z","shell.execute_reply":"2024-11-22T09:17:48.199369Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/11/22 09:17:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## SHUFFLING - SKEWNESS - data distribution across the partitions","metadata":{}},{"cell_type":"markdown","source":"### **1. Run a schuffle *groupByKey* to see how the skew effect computation resources**","metadata":{}},{"cell_type":"code","source":"data_sample = [(1,4),(2,2),(2,1),(3,5),(2,5),(2,10),(2,7),(3,4),(2,1),(2,4),(4,4)]\nrdd_sample = spark.sparkContext.parallelize(data_sample, 3)\nrdd_sample.glom().collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:40:27.779119Z","iopub.execute_input":"2024-11-22T09:40:27.779553Z","iopub.status.idle":"2024-11-22T09:40:30.405277Z","shell.execute_reply.started":"2024-11-22T09:40:27.779515Z","shell.execute_reply":"2024-11-22T09:40:30.404270Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[[(1, 4), (2, 2), (2, 1)],\n [(3, 5), (2, 5), (2, 10)],\n [(2, 7), (3, 4), (2, 1), (2, 4), (4, 4)]]"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"rdd_sample_grouped = rdd_sample.groupByKey()\n\nfor item in rdd_sample_grouped.collect():\n    print(item[0], [value for value in item[1]])\n\n# output:\n# 3 [5, 4]\n# 1 [4]\n# 4 [4]\n# 2 [2, 1, 5, 10, 7, 1, 4] --> Skewness uneven data distribution\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:44:30.360905Z","iopub.execute_input":"2024-11-22T09:44:30.361787Z","iopub.status.idle":"2024-11-22T09:44:31.770511Z","shell.execute_reply.started":"2024-11-22T09:44:30.361730Z","shell.execute_reply":"2024-11-22T09:44:31.767235Z"}},"outputs":[{"name":"stderr","text":"[Stage 1:>                                                          (0 + 3) / 3]\r","output_type":"stream"},{"name":"stdout","text":"3 [5, 4]\n1 [4]\n4 [4]\n2 [2, 1, 5, 10, 7, 1, 4]\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# show partitions\nrdd_sample_grouped.glom().collect()\n\n# output:\n# [[(3, <pyspark.resultiterable.ResultIterable at 0x7cb6b85026e0>)],\n#  [(1, <pyspark.resultiterable.ResultIterable at 0x7cb6b8502ec0>),\n#   (4, <pyspark.resultiterable.ResultIterable at 0x7cb6b8503760>)],\n#  [(2, <pyspark.resultiterable.ResultIterable at 0x7cb6b85031f0>)]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:48:11.942377Z","iopub.execute_input":"2024-11-22T09:48:11.942879Z","iopub.status.idle":"2024-11-22T09:48:12.342703Z","shell.execute_reply.started":"2024-11-22T09:48:11.942836Z","shell.execute_reply":"2024-11-22T09:48:12.341754Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[[(3, <pyspark.resultiterable.ResultIterable at 0x7cb6b85026e0>)],\n [(1, <pyspark.resultiterable.ResultIterable at 0x7cb6b8502ec0>),\n  (4, <pyspark.resultiterable.ResultIterable at 0x7cb6b8503760>)],\n [(2, <pyspark.resultiterable.ResultIterable at 0x7cb6b85031f0>)]]"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### **Solution use *SALTING***","metadata":{}},{"cell_type":"code","source":"# redistribute data by adding random value\n\nimport numpy as np\nimport random\n\nkey_1 = ['a'] * 10\nkey_2 = ['b'] * 6000000\nkey_3 = ['c'] * 800\nkey_4 = ['d'] * 10000\n\nkeys = key_1 + key_2 + key_3 + key_4\n\nrandom.shuffle(keys)\n\nvalue_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\nvalue_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\nvalue_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\nvalue_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\n\nvalue_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T10:16:39.090171Z","iopub.execute_input":"2024-11-22T10:16:39.090571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}