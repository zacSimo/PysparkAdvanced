{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:04:20.759675Z","iopub.execute_input":"2024-11-22T13:04:20.760072Z","iopub.status.idle":"2024-11-22T13:05:05.811175Z","shell.execute_reply.started":"2024-11-22T13:04:20.760037Z","shell.execute_reply":"2024-11-22T13:05:05.809953Z"}},"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=3345aff549add9f14c6f362b72128a10a25d6785152b2a841a896066521fb2e8\n  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"pyspark_practice_4\").getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:05.813732Z","iopub.execute_input":"2024-11-22T13:05:05.814083Z","iopub.status.idle":"2024-11-22T13:05:11.177979Z","shell.execute_reply.started":"2024-11-22T13:05:05.814049Z","shell.execute_reply":"2024-11-22T13:05:11.176965Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/11/22 13:05:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"spark.sparkContext.setLogLevel(\"ERROR\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:11.179739Z","iopub.execute_input":"2024-11-22T13:05:11.181013Z","iopub.status.idle":"2024-11-22T13:05:11.188023Z","shell.execute_reply.started":"2024-11-22T13:05:11.180955Z","shell.execute_reply":"2024-11-22T13:05:11.187079Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## SHUFFLING - SKEWNESS - data distribution across the partitions","metadata":{}},{"cell_type":"markdown","source":"### **1. Run a schuffle *groupByKey* to see how the skew effect computation resources**","metadata":{}},{"cell_type":"code","source":"data_sample = [(1,4),(2,2),(2,1),(3,5),(2,5),(2,10),(2,7),(3,4),(2,1),(2,4),(4,4)]\nrdd_sample = spark.sparkContext.parallelize(data_sample, 3)\nrdd_sample.glom().collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:11.189427Z","iopub.execute_input":"2024-11-22T13:05:11.189796Z","iopub.status.idle":"2024-11-22T13:05:13.635311Z","shell.execute_reply.started":"2024-11-22T13:05:11.189765Z","shell.execute_reply":"2024-11-22T13:05:13.634295Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[[(1, 4), (2, 2), (2, 1)],\n [(3, 5), (2, 5), (2, 10)],\n [(2, 7), (3, 4), (2, 1), (2, 4), (4, 4)]]"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"rdd_sample_grouped = rdd_sample.groupByKey()\n\nfor item in rdd_sample_grouped.collect():\n    print(item[0], [value for value in item[1]])\n\n# Output:\n\n# 3 [5, 4]\n# 1 [4]\n# 4 [4]\n# 2 [2, 1, 5, 10, 7, 1, 4] --> Skewness uneven data distribution\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:13.638953Z","iopub.execute_input":"2024-11-22T13:05:13.639526Z","iopub.status.idle":"2024-11-22T13:05:15.084856Z","shell.execute_reply.started":"2024-11-22T13:05:13.639477Z","shell.execute_reply":"2024-11-22T13:05:15.083797Z"}},"outputs":[{"name":"stderr","text":"[Stage 1:>                                                          (0 + 3) / 3]\r","output_type":"stream"},{"name":"stdout","text":"3 [5, 4]\n1 [4]\n4 [4]\n2 [2, 1, 5, 10, 7, 1, 4]\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# show partitions (result = 4)\nrdd_sample_grouped.glom().collect()\n\n# output:\n# [[(3, <pyspark.resultiterable.ResultIterable at 0x7cb6b85026e0>)],\n#  [(1, <pyspark.resultiterable.ResultIterable at 0x7cb6b8502ec0>),\n#   (4, <pyspark.resultiterable.ResultIterable at 0x7cb6b8503760>)],\n#  [(2, <pyspark.resultiterable.ResultIterable at 0x7cb6b85031f0>)]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:15.086614Z","iopub.execute_input":"2024-11-22T13:05:15.087032Z","iopub.status.idle":"2024-11-22T13:05:15.493856Z","shell.execute_reply.started":"2024-11-22T13:05:15.086984Z","shell.execute_reply":"2024-11-22T13:05:15.492850Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[[(3, <pyspark.resultiterable.ResultIterable at 0x7e78857ec5e0>)],\n [(1, <pyspark.resultiterable.ResultIterable at 0x7e78857ecaf0>),\n  (4, <pyspark.resultiterable.ResultIterable at 0x7e78857ecb50>)],\n [(2, <pyspark.resultiterable.ResultIterable at 0x7e78857ecbb0>)]]"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### **Solution use *SALTING***","metadata":{}},{"cell_type":"code","source":"# redistribute data by adding random value\n\nimport numpy as np\nimport random\n\n# generate skew values\nkey_1 = ['a'] * 10\nkey_2 = ['b'] * 6000000\nkey_3 = ['c'] * 800\nkey_4 = ['d'] * 10000\n\nkeys = key_1 + key_2 + key_3 + key_4\n\nrandom.shuffle(keys)\n\nvalue_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\nvalue_2 = list(np.random.randint(low = 1, high = 100, size = len(key_2)))\nvalue_3 = list(np.random.randint(low = 1, high = 100, size = len(key_3)))\nvalue_4 = list(np.random.randint(low = 1, high = 100, size = len(key_4)))\n\nvalues = value_1 + value_2 + value_3 + value_4\n\npair_skew = list(zip(keys, values))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:15.494800Z","iopub.execute_input":"2024-11-22T13:05:15.495150Z","iopub.status.idle":"2024-11-22T13:05:20.888354Z","shell.execute_reply.started":"2024-11-22T13:05:15.495105Z","shell.execute_reply":"2024-11-22T13:05:20.887260Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# load skew values into RDD\nrdd = spark.sparkContext.parallelize(pair_skew, 8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:20.889946Z","iopub.execute_input":"2024-11-22T13:05:20.890290Z","iopub.status.idle":"2024-11-22T13:05:44.274073Z","shell.execute_reply.started":"2024-11-22T13:05:20.890259Z","shell.execute_reply":"2024-11-22T13:05:44.272985Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"%time\nrdd_group = rdd.groupByKey().cache()\n# run a simple data transformation on skewed data and see the result on cluster DAG\nrdd_group.map(lambda pair: (pair[0], [i for i in pair[1]])).count()\n\n# Output:\n# 4 --> we have 4 partitions with value that represent number of keys in groupByKey","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:44.275267Z","iopub.execute_input":"2024-11-22T13:05:44.275580Z","iopub.status.idle":"2024-11-22T13:06:34.284176Z","shell.execute_reply.started":"2024-11-22T13:05:44.275550Z","shell.execute_reply":"2024-11-22T13:06:34.283206Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 6.91 µs\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### *groupByKey* **Migrate data skewness: *SALTING***","metadata":{}},{"cell_type":"code","source":"#  We define the salting method for data redistribution\ndef salting(val):\n    tmp = val + \"_\" + str(random.randint(0,5)) # increasing the random ('high') we have better results \n    return tmp\n\nsalting(\"10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:06:34.285374Z","iopub.execute_input":"2024-11-22T13:06:34.285785Z","iopub.status.idle":"2024-11-22T13:06:34.296046Z","shell.execute_reply.started":"2024-11-22T13:06:34.285743Z","shell.execute_reply":"2024-11-22T13:06:34.295076Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'10_5'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Apply salting method to skewed data for data redistribution\n%time\nrdd_salting = rdd.map(lambda x: (salting(x[0]), x[1]))\n\nrdd_grouped = rdd_salting.groupByKey().cache()\n# run a simple data transformation on skewed data and see the result on cluster (job/DAG/stage)\nrdd_grouped.map(lambda pair: (pair[0], [i for i in pair[1]])).count()\n\n# Output:\n# 23 --> we get 23 partitions after repartitioning with randaom data\n# increasing the random value ('high') in 'salting' method, we can have better results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:06:34.297961Z","iopub.execute_input":"2024-11-22T13:06:34.298771Z","iopub.status.idle":"2024-11-22T13:07:18.381692Z","shell.execute_reply.started":"2024-11-22T13:06:34.298722Z","shell.execute_reply":"2024-11-22T13:07:18.380137Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 7.39 µs\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### **2. Run a schuffle *sortByKey* to see how the skew effect computation resources**","metadata":{}},{"cell_type":"code","source":"rdd_sort = rdd.sortByKey(ascending=False, numPartitions=4)\nrdd_sort.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:07:18.383456Z","iopub.execute_input":"2024-11-22T13:07:18.383838Z","iopub.status.idle":"2024-11-22T13:08:06.319972Z","shell.execute_reply.started":"2024-11-22T13:07:18.383802Z","shell.execute_reply":"2024-11-22T13:08:06.318751Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"6010810"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### *sortByKey* **Migrate data skewness: *SALTING***","metadata":{}},{"cell_type":"code","source":"rdd_sorted = rdd_salting.sortByKey(ascending=False, numPartitions=4)\nrdd_sorted.count()\n\n# Output:\n\n# We have the same size data in the result (6010810)\n# The redistribution data in the partitions are better than the previous one.\n# That is confirmed by the time taken by the computation, 2 sec # 18 sec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:08:06.321453Z","iopub.execute_input":"2024-11-22T13:08:06.321860Z","iopub.status.idle":"2024-11-22T13:08:45.150345Z","shell.execute_reply.started":"2024-11-22T13:08:06.321819Z","shell.execute_reply":"2024-11-22T13:08:45.149066Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"6010810"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### **3. Run a schuffle *join* to see how the skew effect computation resources**","metadata":{}},{"cell_type":"code","source":"small_rdd1 = spark.sparkContext.parallelize([(2,3),(1,3),(1,4),(3,1),(5,1)], 3)\nsmall_rdd2 = spark.sparkContext.parallelize([(3,4),(0,1),(1,2),(2,1)], 2)\n\nprint(small_rdd1.collect())\nprint(small_rdd2.collect())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:11:44.778014Z","iopub.execute_input":"2024-11-22T13:11:44.779050Z","iopub.status.idle":"2024-11-22T13:11:44.849996Z","shell.execute_reply.started":"2024-11-22T13:11:44.779010Z","shell.execute_reply":"2024-11-22T13:11:44.848711Z"}},"outputs":[{"name":"stdout","text":"[(2, 3), (1, 3), (1, 4), (3, 1), (5, 1)]\n[(3, 4), (0, 1), (1, 2), (2, 1)]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"join1 = small_rdd1.join(small_rdd2)\njoin1.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:13:16.793147Z","iopub.execute_input":"2024-11-22T13:13:16.793628Z","iopub.status.idle":"2024-11-22T13:13:18.160161Z","shell.execute_reply.started":"2024-11-22T13:13:16.793584Z","shell.execute_reply":"2024-11-22T13:13:18.158887Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[(1, (3, 2)), (1, (4, 2)), (2, (3, 1)), (3, (1, 4))]"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"join1.getNumPartitions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:14:41.014280Z","iopub.execute_input":"2024-11-22T13:14:41.015209Z","iopub.status.idle":"2024-11-22T13:14:41.022690Z","shell.execute_reply.started":"2024-11-22T13:14:41.015166Z","shell.execute_reply":"2024-11-22T13:14:41.021700Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"join1.glom().collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:15:01.006926Z","iopub.execute_input":"2024-11-22T13:15:01.007351Z","iopub.status.idle":"2024-11-22T13:15:01.512421Z","shell.execute_reply.started":"2024-11-22T13:15:01.007317Z","shell.execute_reply":"2024-11-22T13:15:01.511285Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[[], [(1, (3, 2)), (1, (4, 2))], [(2, (3, 1))], [(3, (1, 4))], []]"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# generate skew values for 'join'\nkey_1 = ['a'] * 5\nkey_2 = ['b'] * 60\nkey_3 = ['c'] * 100\n\nkeys = key_1 + key_2 + key_3\n\nrandom.shuffle(keys)\n\nvalue_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\nvalue_2 = list(np.random.randint(low = 1, high = 100, size = len(key_2)))\nvalue_3 = list(np.random.randint(low = 1, high = 100, size = len(key_3)))\n\nvalues = value_1 + value_2 + value_3\n\npair_skew_j = list(zip(keys, values))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:17:27.968291Z","iopub.execute_input":"2024-11-22T13:17:27.969156Z","iopub.status.idle":"2024-11-22T13:17:27.976001Z","shell.execute_reply.started":"2024-11-22T13:17:27.969114Z","shell.execute_reply":"2024-11-22T13:17:27.974785Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"small_rdd = spark.sparkContext.parallelize(pair_skew_j, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:39:00.523454Z","iopub.execute_input":"2024-11-22T13:39:00.523971Z","iopub.status.idle":"2024-11-22T13:39:00.534943Z","shell.execute_reply.started":"2024-11-22T13:39:00.523924Z","shell.execute_reply":"2024-11-22T13:39:00.533775Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"small_rdd.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:39:16.459706Z","iopub.execute_input":"2024-11-22T13:39:16.460148Z","iopub.status.idle":"2024-11-22T13:39:16.710574Z","shell.execute_reply.started":"2024-11-22T13:39:16.460114Z","shell.execute_reply":"2024-11-22T13:39:16.708332Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"165"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"join1.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:39:34.243660Z","iopub.execute_input":"2024-11-22T13:39:34.244107Z","iopub.status.idle":"2024-11-22T13:39:34.743759Z","shell.execute_reply.started":"2024-11-22T13:39:34.244075Z","shell.execute_reply":"2024-11-22T13:39:34.742724Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# join without salting\n%time\nrdd_j = rdd.join(small_rdd)\nrdd_j.map(lambda x: int(x[1][0] + x[1][1])).reduce(lambda x,y: x+y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:05:51.389438Z","iopub.execute_input":"2024-11-22T14:05:51.390042Z","iopub.status.idle":"2024-11-22T14:11:48.282823Z","shell.execute_reply.started":"2024-11-22T14:05:51.390006Z","shell.execute_reply":"2024-11-22T14:11:48.281755Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 10.7 µs\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"37061489135"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"### *join* **Migrate data skewness: *SALTING***","metadata":{}},{"cell_type":"code","source":"# add a random value to the key --> (key,randint)\nrdd_new = rdd.map(lambda x: ((x[0], random.randint(0, 10)), x[1])).cache() # increase random value for better results\n\n# Replicate the small data\nsmall_rdd_new = small_rdd.cartesian(spark.sparkContext.parallelize(range(0, 11))).map(lambda x: ((x[0][0], x[1]), x[0][1])).cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:51:10.191144Z","iopub.execute_input":"2024-11-22T13:51:10.191602Z","iopub.status.idle":"2024-11-22T13:51:10.237675Z","shell.execute_reply.started":"2024-11-22T13:51:10.191560Z","shell.execute_reply":"2024-11-22T13:51:10.236479Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# join with salting\n%time\nrdd_join = rdd.join(small_rdd_new)\nrdd_join.map(lambda x: int(x[1][0] + x[1][1])).reduce(lambda x,y: x+y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:21:07.814466Z","iopub.execute_input":"2024-11-22T14:21:07.815106Z","iopub.status.idle":"2024-11-22T14:22:25.478759Z","shell.execute_reply.started":"2024-11-22T14:21:07.815066Z","shell.execute_reply":"2024-11-22T14:22:25.476728Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 4 µs, sys: 0 ns, total: 4 µs\nWall time: 11.7 µs\n","output_type":"stream"},{"name":"stderr","text":"24/11/22 14:22:24 ERROR Executor: Exception in task 4.0 in stage 36.0 (TID 233)]\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 1919, in func\n    initial = next(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 366, in _external_items\n    self._spill()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 793, in _spill\n    sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))\nTypeError: '<' not supported between instances of 'tuple' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/11/22 14:22:24 ERROR TaskSetManager: Task 4 in stage 36.0 failed 1 times; aborting job\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m rdd_join \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mjoin(small_rdd_new)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrdd_join\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 36.0 failed 1 times, most recent failure: Lost task 4.0 in stage 36.0 (TID 233) (1508a62614cd executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 1919, in func\n    initial = next(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 366, in _external_items\n    self._spill()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 793, in _spill\n    sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))\nTypeError: '<' not supported between instances of 'tuple' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 1919, in func\n    initial = next(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 366, in _external_items\n    self._spill()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 793, in _spill\n    sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))\nTypeError: '<' not supported between instances of 'tuple' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"],"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 36.0 failed 1 times, most recent failure: Lost task 4.0 in stage 36.0 (TID 233) (1508a62614cd executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 1919, in func\n    initial = next(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 366, in _external_items\n    self._spill()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 793, in _spill\n    sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))\nTypeError: '<' not supported between instances of 'tuple' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 1919, in func\n    initial = next(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 366, in _external_items\n    self._spill()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 793, in _spill\n    sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))\nTypeError: '<' not supported between instances of 'tuple' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n","output_type":"error"}],"execution_count":32},{"cell_type":"markdown","source":"### **4. Loading data Skew**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n\n# sale dataset:\n# table 1: OrderId, Qty, Sales, Discount, (yes=1, no=0)\n# table 2: ProductID, OrderID, Product, Price\n\n#################### Table 1 #####################\n\nkey_1 = [101] * 1\nkey_2 = [201] * 8\nkey_3 = [301] * 4\nkey_4 = [401] * 2\n\nOrderId = key_1 + key_2 + key_3 + key_4\n\nrandom.shuffle(OrderId)\n\nQty_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\nQty_2 = list(np.random.randint(low = 1, high = 200, size = len(key_2)))\nQty_3 = list(np.random.randint(low = 1, high = 1000, size = len(key_3)))\nQty_4 = list(np.random.randint(low = 1, high = 50, size = len(key_4)))\n\nQty = Qty_1 + Qty_2 + Qty_3 + Qty_4\n\nSales_1 = list(np.random.randint(low = 1, high = 100, size = len(key_1)))\nSales_2 = list(np.random.randint(low = 1, high = 3400, size = len(key_2)))\nSales_3 = list(np.random.randint(low = 1, high = 2000, size = len(key_3)))\nSales_4 = list(np.random.randint(low = 1, high = 1000, size = len(key_4)))\n\nSales = Sales_1 + Sales_2 + Sales_3 + Sales_4\n\nDiscount = list(np.random.randint(low = 1, high = 2, size = len(OrderId)))\ndata1 = list(zip(OrderId, Qty, Sales, Discount))\n\n# create pandas dataFrame\ndata_skew = pd.DataFrame(data1, columns=['OrderID', 'Qty', 'Sales', 'Discount'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:20:40.919875Z","iopub.execute_input":"2024-11-22T14:20:40.920403Z","iopub.status.idle":"2024-11-22T14:20:42.422670Z","shell.execute_reply.started":"2024-11-22T14:20:40.920364Z","shell.execute_reply":"2024-11-22T14:20:42.421522Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"#################### Table 2 #####################\ndata2 = [[1, 101, 'pencil', 4.99],\n         [2, 101, 'book', 9.5],\n         [3, 101, 'scissors', 14],\n         [4, 301, 'glue', 7],\n         [5, 201, 'marker', 8.49],\n         [6, 301, 'label', 2],\n         [7, 201, 'calculator', 3.99],\n         [8, 501, 'eraser', 1.55],\n        ]\n\ndata_small = pd.DataFrame(data2, columns=['ProductID', 'OrderID', 'Product', 'Price'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:23:26.078687Z","iopub.execute_input":"2024-11-22T14:23:26.079095Z","iopub.status.idle":"2024-11-22T14:23:26.086195Z","shell.execute_reply.started":"2024-11-22T14:23:26.079066Z","shell.execute_reply":"2024-11-22T14:23:26.085083Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# create pyspark DF from Pandas\n# Optimize conversion between pySpark and Pandas DF: Enable arrow-based columar data transfers\n\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n\ndf_skew = spark.createDataFrame(data_skew)\ndf_skew.printSchema()\ndf_skew.show()\nprint(df_skew.rdd.getNumPartitions())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:28:38.043123Z","iopub.execute_input":"2024-11-22T14:28:38.043533Z","iopub.status.idle":"2024-11-22T14:28:38.522389Z","shell.execute_reply.started":"2024-11-22T14:28:38.043499Z","shell.execute_reply":"2024-11-22T14:28:38.521340Z"}},"outputs":[{"name":"stdout","text":"root\n |-- OrderID: long (nullable = true)\n |-- Qty: long (nullable = true)\n |-- Sales: long (nullable = true)\n |-- Discount: long (nullable = true)\n\n+-------+---+-----+--------+\n|OrderID|Qty|Sales|Discount|\n+-------+---+-----+--------+\n|    201| 27|   66|       1|\n|    201|143| 2356|       1|\n|    201| 66|   87|       1|\n|    301|107| 1827|       1|\n|    201| 16| 2019|       1|\n|    101|112|  512|       1|\n|    301|  5| 2570|       1|\n|    201|169| 2790|       1|\n|    301|  1|  901|       1|\n|    401|863|  890|       1|\n|    301|119|  475|       1|\n|    401|765|  613|       1|\n|    201|260| 1968|       1|\n|    201| 27|  305|       1|\n|    201|  7|   36|       1|\n+-------+---+-----+--------+\n\n4\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"df_small = spark.createDataFrame(data_small)\ndf_small.printSchema()\ndf_small.show()\ndf_small.rdd.getNumPartitions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:40:38.843334Z","iopub.execute_input":"2024-11-22T14:40:38.844356Z","iopub.status.idle":"2024-11-22T14:40:38.937200Z","shell.execute_reply.started":"2024-11-22T14:40:38.844314Z","shell.execute_reply":"2024-11-22T14:40:38.936128Z"}},"outputs":[{"name":"stdout","text":"root\n |-- ProductID: long (nullable = true)\n |-- OrderID: long (nullable = true)\n |-- Product: string (nullable = true)\n |-- Price: double (nullable = true)\n\n+---------+-------+----------+-----+\n|ProductID|OrderID|   Product|Price|\n+---------+-------+----------+-----+\n|        1|    101|    pencil| 4.99|\n|        2|    101|      book|  9.5|\n|        3|    101|  scissors| 14.0|\n|        4|    301|      glue|  7.0|\n|        5|    201|    marker| 8.49|\n|        6|    301|     label|  2.0|\n|        7|    201|calculator| 3.99|\n|        8|    501|    eraser| 1.55|\n+---------+-------+----------+-----+\n\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"### **4.1. Run a schuffle *join* with small size of data**","metadata":{}},{"cell_type":"code","source":"joined_df = df_skew.join(df_small, df_skew.OrderID == df_small.OrderID, how=\"inner\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:46:16.627693Z","iopub.execute_input":"2024-11-22T14:46:16.628218Z","iopub.status.idle":"2024-11-22T14:46:16.647569Z","shell.execute_reply.started":"2024-11-22T14:46:16.628167Z","shell.execute_reply":"2024-11-22T14:46:16.646429Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"joined_df.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:46:17.455644Z","iopub.execute_input":"2024-11-22T14:46:17.456162Z","iopub.status.idle":"2024-11-22T14:46:17.782186Z","shell.execute_reply.started":"2024-11-22T14:46:17.456115Z","shell.execute_reply":"2024-11-22T14:46:17.781029Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"27"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"joined_df.show(30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:46:18.093318Z","iopub.execute_input":"2024-11-22T14:46:18.093772Z","iopub.status.idle":"2024-11-22T14:46:18.302949Z","shell.execute_reply.started":"2024-11-22T14:46:18.093735Z","shell.execute_reply":"2024-11-22T14:46:18.299479Z"}},"outputs":[{"name":"stdout","text":"+-------+---+-----+--------+---------+-------+----------+-----+\n|OrderID|Qty|Sales|Discount|ProductID|OrderID|   Product|Price|\n+-------+---+-----+--------+---------+-------+----------+-----+\n|    201| 27|   66|       1|        7|    201|calculator| 3.99|\n|    201| 27|   66|       1|        5|    201|    marker| 8.49|\n|    201|143| 2356|       1|        7|    201|calculator| 3.99|\n|    201|143| 2356|       1|        5|    201|    marker| 8.49|\n|    201| 66|   87|       1|        7|    201|calculator| 3.99|\n|    201| 66|   87|       1|        5|    201|    marker| 8.49|\n|    301|107| 1827|       1|        6|    301|     label|  2.0|\n|    301|107| 1827|       1|        4|    301|      glue|  7.0|\n|    201| 16| 2019|       1|        7|    201|calculator| 3.99|\n|    201| 16| 2019|       1|        5|    201|    marker| 8.49|\n|    101|112|  512|       1|        3|    101|  scissors| 14.0|\n|    101|112|  512|       1|        2|    101|      book|  9.5|\n|    101|112|  512|       1|        1|    101|    pencil| 4.99|\n|    301|  5| 2570|       1|        6|    301|     label|  2.0|\n|    301|  5| 2570|       1|        4|    301|      glue|  7.0|\n|    201|169| 2790|       1|        7|    201|calculator| 3.99|\n|    201|169| 2790|       1|        5|    201|    marker| 8.49|\n|    301|  1|  901|       1|        6|    301|     label|  2.0|\n|    301|  1|  901|       1|        4|    301|      glue|  7.0|\n|    301|119|  475|       1|        6|    301|     label|  2.0|\n|    301|119|  475|       1|        4|    301|      glue|  7.0|\n|    201|260| 1968|       1|        7|    201|calculator| 3.99|\n|    201|260| 1968|       1|        5|    201|    marker| 8.49|\n|    201| 27|  305|       1|        7|    201|calculator| 3.99|\n|    201| 27|  305|       1|        5|    201|    marker| 8.49|\n|    201|  7|   36|       1|        7|    201|calculator| 3.99|\n|    201|  7|   36|       1|        5|    201|    marker| 8.49|\n+-------+---+-----+--------+---------+-------+----------+-----+\n\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"joined_df.rdd.getNumPartitions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:49:05.238594Z","iopub.execute_input":"2024-11-22T14:49:05.238999Z","iopub.status.idle":"2024-11-22T14:49:05.246726Z","shell.execute_reply.started":"2024-11-22T14:49:05.238966Z","shell.execute_reply":"2024-11-22T14:49:05.245596Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"joined_df.rdd.glom().collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:47:42.164839Z","iopub.execute_input":"2024-11-22T14:47:42.165301Z","iopub.status.idle":"2024-11-22T14:47:42.739504Z","shell.execute_reply.started":"2024-11-22T14:47:42.165268Z","shell.execute_reply":"2024-11-22T14:47:42.738356Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"[[Row(OrderID=201, Qty=27, Sales=66, Discount=1, ProductID=7, OrderID=201, Product='calculator', Price=3.99),\n  Row(OrderID=201, Qty=27, Sales=66, Discount=1, ProductID=5, OrderID=201, Product='marker', Price=8.49),\n  Row(OrderID=201, Qty=143, Sales=2356, Discount=1, ProductID=7, OrderID=201, Product='calculator', Price=3.99),\n  Row(OrderID=201, Qty=143, Sales=2356, Discount=1, ProductID=5, OrderID=201, Product='marker', Price=8.49),\n  Row(OrderID=201, Qty=66, Sales=87, Discount=1, ProductID=7, OrderID=201, Product='calculator', Price=3.99),\n  Row(OrderID=201, Qty=66, Sales=87, Discount=1, ProductID=5, OrderID=201, Product='marker', Price=8.49)],\n [Row(OrderID=301, Qty=107, Sales=1827, Discount=1, ProductID=6, OrderID=301, Product='label', Price=2.0),\n  Row(OrderID=301, Qty=107, Sales=1827, Discount=1, ProductID=4, OrderID=301, Product='glue', Price=7.0),\n  Row(OrderID=201, Qty=16, Sales=2019, Discount=1, ProductID=7, OrderID=201, Product='calculator', Price=3.99),\n  Row(OrderID=201, Qty=16, Sales=2019, Discount=1, ProductID=5, OrderID=201, Product='marker', Price=8.49),\n  Row(OrderID=101, Qty=112, Sales=512, Discount=1, ProductID=3, OrderID=101, Product='scissors', Price=14.0),\n  Row(OrderID=101, Qty=112, Sales=512, Discount=1, ProductID=2, OrderID=101, Product='book', Price=9.5),\n  Row(OrderID=101, Qty=112, Sales=512, Discount=1, ProductID=1, OrderID=101, Product='pencil', Price=4.99),\n  Row(OrderID=301, Qty=5, Sales=2570, Discount=1, ProductID=6, OrderID=301, Product='label', Price=2.0),\n  Row(OrderID=301, Qty=5, Sales=2570, Discount=1, ProductID=4, OrderID=301, Product='glue', Price=7.0)],\n [Row(OrderID=201, Qty=169, Sales=2790, Discount=1, ProductID=7, OrderID=201, Product='calculator', Price=3.99),\n  Row(OrderID=201, Qty=169, Sales=2790, Discount=1, ProductID=5, OrderID=201, Product='marker', Price=8.49),\n  Row(OrderID=301, Qty=1, Sales=901, Discount=1, ProductID=6, OrderID=301, Product='label', Price=2.0),\n  Row(OrderID=301, Qty=1, Sales=901, Discount=1, ProductID=4, OrderID=301, Product='glue', Price=7.0),\n  Row(OrderID=301, Qty=119, Sales=475, Discount=1, ProductID=6, OrderID=301, Product='label', Price=2.0),\n  Row(OrderID=301, Qty=119, Sales=475, Discount=1, ProductID=4, OrderID=301, Product='glue', Price=7.0)],\n [Row(OrderID=201, Qty=260, Sales=1968, Discount=1, ProductID=7, OrderID=201, Product='calculator', Price=3.99),\n  Row(OrderID=201, Qty=260, Sales=1968, Discount=1, ProductID=5, OrderID=201, Product='marker', Price=8.49),\n  Row(OrderID=201, Qty=27, Sales=305, Discount=1, ProductID=7, OrderID=201, Product='calculator', Price=3.99),\n  Row(OrderID=201, Qty=27, Sales=305, Discount=1, ProductID=5, OrderID=201, Product='marker', Price=8.49),\n  Row(OrderID=201, Qty=7, Sales=36, Discount=1, ProductID=7, OrderID=201, Product='calculator', Price=3.99),\n  Row(OrderID=201, Qty=7, Sales=36, Discount=1, ProductID=5, OrderID=201, Product='marker', Price=8.49)]]"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}